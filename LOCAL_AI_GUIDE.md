# ğŸ§  æœ¬åœ° AI è¨“ç·´æŒ‡å— - ä¸ä½¿ç”¨ OpenAI

æœ¬æŒ‡å—å°‡æ•™ä½ å¦‚ä½•ä½¿ç”¨æœ¬åœ° AI æ¨¡å‹é‹è¡Œ Project Observerï¼Œå®Œå…¨ä¸ä¾è³´ OpenAI APIã€‚

## ç›®éŒ„
- [ç‚ºä»€éº¼ä½¿ç”¨æœ¬åœ° AI](#ç‚ºä»€éº¼ä½¿ç”¨æœ¬åœ°-ai)
- [é¸æ“‡æœ¬åœ°æ¨¡å‹](#é¸æ“‡æœ¬åœ°æ¨¡å‹)
- [Ollama å¿«é€Ÿè¨­ç½®](#ollama-å¿«é€Ÿè¨­ç½®)
- [é€²éšï¼šè¨“ç·´è‡ªå·±çš„æ¨¡å‹](#é€²éšè¨“ç·´è‡ªå·±çš„æ¨¡å‹)
- [æ€§èƒ½å„ªåŒ–](#æ€§èƒ½å„ªåŒ–)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ç‚ºä»€éº¼ä½¿ç”¨æœ¬åœ° AI

âœ… **å„ªå‹¢**ï¼š
- ğŸ’° **å®Œå…¨å…è²»** - ç„¡éœ€æ”¯ä»˜ API è²»ç”¨
- ğŸ”’ **æ•¸æ“šéš±ç§** - æ‰€æœ‰æ•¸æ“šä¿ç•™åœ¨æœ¬åœ°
- âš¡ **ç„¡é™èª¿ç”¨** - æ²’æœ‰é€Ÿç‡é™åˆ¶
- ğŸ¯ **å¯å®šåˆ¶** - å¯ä»¥å¾®èª¿å°ˆå±¬æ¨¡å‹
- ğŸŒ **é›¢ç·šé‹è¡Œ** - ä¸éœ€è¦ç¶²è·¯é€£æ¥

âš ï¸ **è€ƒé‡**ï¼š
- éœ€è¦è¼ƒé«˜çš„ç¡¬é«”é…ç½®ï¼ˆGPU æ¨è–¦ï¼‰
- åˆæ¬¡ä¸‹è¼‰æ¨¡å‹éœ€è¦æ™‚é–“
- æ¨ç†é€Ÿåº¦å¯èƒ½è¼ƒæ…¢ï¼ˆå–æ±ºæ–¼ç¡¬é«”ï¼‰

---

## é¸æ“‡æœ¬åœ°æ¨¡å‹

### æ¨è–¦æ¨¡å‹ï¼ˆæŒ‰æ€§èƒ½æ’åºï¼‰

| æ¨¡å‹ | åƒæ•¸é‡ | è¨˜æ†¶é«”éœ€æ±‚ | æ¨è–¦ç”¨é€” | ä¸‹è¼‰å¤§å° |
|------|--------|-----------|---------|----------|
| **Llama 3.1 70B** | 70B | 48GB+ | æœ€ä½³æ€§èƒ½ | 40GB |
| **Llama 3.1 8B** | 8B | 8GB | å¹³è¡¡é¸æ“‡ | 4.7GB |
| **Llama 3.2 3B** | 3B | 4GB | å¿«é€Ÿæ¨ç† | 2GB |
| **Phi-3 Mini** | 3.8B | 4GB | è¼•é‡ç´š | 2.3GB |
| **Mistral 7B** | 7B | 8GB | é«˜è³ªé‡ | 4.1GB |
| **Qwen 2.5** | 7B | 8GB | ä¸­æ–‡å„ªåŒ– | 4.4GB |

### ç¡¬é«”éœ€æ±‚å°ç…§

- **åŸºç¤é…ç½®**ï¼š8GB RAM + CPU â†’ Llama 3.2 3B
- **æ¨è–¦é…ç½®**ï¼š16GB RAM + RTX 3060 â†’ Llama 3.1 8B
- **é«˜éšé…ç½®**ï¼š32GB RAM + RTX 4090 â†’ Llama 3.1 70B

---

## Ollama å¿«é€Ÿè¨­ç½®

### 1. å®‰è£ Ollama

#### Linux / macOS
```bash
# ä¸€éµå®‰è£
curl -fsSL https://ollama.com/install.sh | sh

# é©—è­‰å®‰è£
ollama --version
```

#### Windows
```powershell
# ä¸‹è¼‰ä¸¦å®‰è£ Ollama for Windows
# https://ollama.com/download/windows

# æˆ–ä½¿ç”¨ WSL2
wsl -d Ubuntu
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. ä¸‹è¼‰æ¨¡å‹

```bash
# æ¨è–¦ï¼šLlama 3.1 8B (å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦)
ollama pull llama3.1:8b

# æˆ–é¸æ“‡å…¶ä»–æ¨¡å‹
ollama pull llama3.2:3b      # æ›´å¿«ï¼Œè¼ƒå°
ollama pull mistral:7b       # é«˜è³ªé‡
ollama pull qwen2.5:7b       # ä¸­æ–‡å‹å¥½
ollama pull phi3:mini        # è¼•é‡ç´š
```

### 3. å•Ÿå‹• Ollama æœå‹™

```bash
# å‰å°é‹è¡Œï¼ˆæŸ¥çœ‹æ—¥èªŒï¼‰
ollama serve

# æˆ–èƒŒæ™¯é‹è¡Œ
nohup ollama serve > ollama.log 2>&1 &
```

### 4. æ¸¬è©¦æ¨¡å‹

```bash
# äº¤äº’å¼æ¸¬è©¦
ollama run llama3.1:8b

# æ¸¬è©¦æç¤º
ollama run llama3.1:8b "ä½ æ˜¯ä¸€å€‹åœ¨ Minecraft ä¸–ç•Œä¸­ç”Ÿå­˜çš„ AIã€‚ç¾åœ¨ä½ çœ‹åˆ°å‰æ–¹æœ‰ä¸€æ£µæ¨¹ï¼Œä½ æ‡‰è©²åšä»€éº¼ï¼Ÿ"
```

### 5. é…ç½® Project Observer

ç·¨è¼¯ `.env` æ–‡ä»¶ï¼š

```bash
# åœç”¨ OpenAI
# OPENAI_API_KEY=sk-xxxxx

# å•Ÿç”¨æœ¬åœ° Ollama
OPENAI_API_BASE=http://host.docker.internal:11434/v1
LLM_MODEL=llama3.1:8b
OPENAI_API_KEY=ollama  # ä»»æ„å€¼å³å¯

# å¯é¸ï¼šèª¿æ•´æ¨ç†åƒæ•¸
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1000
```

### 6. å•Ÿå‹•ç³»çµ±

```bash
# ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œ
ollama serve &

# å•Ÿå‹• Project Observer
make start

# æŸ¥çœ‹æ—¥èªŒç¢ºèªä½¿ç”¨æœ¬åœ°æ¨¡å‹
make logs-ai | grep "LLM"
```

---

## é€²éšï¼šè¨“ç·´è‡ªå·±çš„æ¨¡å‹

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ Ollama Modelfile è‡ªå®šç¾©

å‰µå»ºå°ˆå±¬çš„ Minecraft AI æ¨¡å‹ï¼š

```bash
# 1. å‰µå»º Modelfile
cat > MinecraftAgent.modelfile << 'EOF'
FROM llama3.1:8b

# è¨­ç½®ç³»çµ±æç¤ºè©
SYSTEM """
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ Minecraft ç”Ÿå­˜å°ˆå®¶ AIã€‚ä½ çš„ç›®æ¨™æ˜¯ï¼š
1. é«˜æ•ˆåœ°æ”¶é›†è³‡æº
2. å»ºé€ å®‰å…¨çš„åº‡è­·æ‰€
3. è£½ä½œé€²éšå·¥å…·å’Œè£å‚™
4. æ¢ç´¢ä¸¦å¾æœä¸åŒçš„ç”Ÿç‰©ç¾¤ç³»

ä½ ç¸½æ˜¯ç”¨ JSON æ ¼å¼å›æ‡‰ï¼ŒåŒ…å«æ¸…æ™°çš„ç›®æ¨™ã€æ¨ç†å’Œä»£ç¢¼ã€‚
ä½ å¾éŒ¯èª¤ä¸­å¿«é€Ÿå­¸ç¿’ï¼Œä¸¦ä¸æ–·å„ªåŒ–ç­–ç•¥ã€‚
"""

# èª¿æ•´åƒæ•¸
PARAMETER temperature 0.8
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_ctx 4096

# æ·»åŠ ç¯„ä¾‹å°è©±
MESSAGE user æˆ‘çœ‹åˆ°å‰æ–¹æœ‰æ¨¹æ—ï¼ŒèƒŒåŒ…æ˜¯ç©ºçš„ã€‚
MESSAGE assistant {"goal": "ç²å–æœ¨é ­è£½ä½œå·¥å…·", "reasoning": "æœ¨é ­æ˜¯æœ€åŸºç¤çš„è³‡æºï¼Œå¯ä»¥è£½ä½œå·¥ä½œå°å’Œæœ¨è£½å·¥å…·", "action_type": "generate_code", "code": "await bot.pathfinder.goto(new goals.GoalNear(treePosition.x, treePosition.y, treePosition.z, 1)); await mineBlock('oak_log');"}

MESSAGE user æˆ‘çš„è¡€é‡åªå‰© 5 é»ï¼Œé™„è¿‘æœ‰æ®­å±ã€‚
MESSAGE assistant {"goal": "é€ƒé›¢å±éšªå€åŸŸ", "reasoning": "ä½è¡€é‡æ™‚æ‡‰é¿å…æˆ°é¬¥ï¼Œå„ªå…ˆç”Ÿå­˜", "action_type": "generate_code", "code": "const safeDistance = 50; await bot.pathfinder.goto(new goals.GoalInvert(new goals.GoalFollow(zombie, safeDistance)));"}
EOF

# 2. å‰µå»ºè‡ªå®šç¾©æ¨¡å‹
ollama create minecraft-agent -f MinecraftAgent.modelfile

# 3. æ¸¬è©¦æ¨¡å‹
ollama run minecraft-agent "æˆ‘åœ¨æ²™æ¼ ä¸­ï¼Œå£æ¸´ä¸”é£¢é¤“ã€‚"

# 4. ä½¿ç”¨è‡ªå®šç¾©æ¨¡å‹
# ä¿®æ”¹ .env
LLM_MODEL=minecraft-agent
```

### æ–¹æ³•äºŒï¼šä½¿ç”¨çœŸå¯¦éŠæˆ²æ•¸æ“šå¾®èª¿

#### æº–å‚™è¨“ç·´æ•¸æ“š

```bash
# å‰µå»ºè¨“ç·´æ•¸æ“šç›®éŒ„
mkdir -p training_data

# æ”¶é›† AI çš„æˆåŠŸæ¡ˆä¾‹
cat > training_data/minecraft_examples.jsonl << 'EOF'
{"prompt": "è§€å¯Ÿï¼šç”Ÿå‘½å€¼20ï¼Œé£¢é¤“å€¼15ï¼Œå‘¨åœæœ‰æ©¡æ¨¹ã€‚ç›®æ¨™æ˜¯ä»€éº¼ï¼Ÿ", "response": "æ”¶é›†æœ¨é ­æ˜¯é¦–è¦ä»»å‹™ã€‚æœ¨é ­å¯ä»¥è£½ä½œå·¥ä½œå°å’Œå·¥å…·ã€‚", "code": "await mineBlock('oak_log');"}
{"prompt": "è§€å¯Ÿï¼šå¤œæ™šï¼Œè¡€é‡10ï¼Œé™„è¿‘æœ‰3éš»æ®­å±ã€‚ç­–ç•¥ï¼Ÿ", "response": "å¤œæ™šä¸”ä½è¡€é‡æ‡‰é¿å…æˆ°é¬¥ï¼Œå°‹æ‰¾é«˜è™•æˆ–å»ºé€ è‡¨æ™‚åº‡è­·æ‰€ã€‚", "code": "await bot.placeBlock(bot.blockAt(bot.entity.position.offset(0, -1, 1)), new Vec3(0, 1, 0));"}
{"prompt": "è§€å¯Ÿï¼šæœ‰å·¥ä½œå°å’Œæœ¨é ­ï¼Œéœ€è¦æ›´å¥½çš„å·¥å…·ã€‚", "response": "è£½ä½œæœ¨é¬ï¼Œç„¶å¾ŒæŒ–çŸ³é ­å‡ç´šåˆ°çŸ³è£½å·¥å…·ã€‚", "code": "await bot.craft(mcData.itemsByName['wooden_pickaxe'], 1);"}
EOF
```

#### ä½¿ç”¨ LLaMA Factory å¾®èª¿

```bash
# 1. å®‰è£ LLaMA Factory
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e .

# 2. æº–å‚™é…ç½®
cat > minecraft_lora_config.yaml << 'EOF'
model_name_or_path: meta-llama/Llama-3.1-8B
stage: sft
do_train: true
finetuning_type: lora
lora_target: all

dataset: minecraft_survival
template: llama3
cutoff_len: 2048
max_samples: 1000
overwrite_cache: true
preprocessing_num_workers: 16

output_dir: minecraft_agent_lora
logging_steps: 10
save_steps: 100
plot_loss: true

per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 5.0e-5
num_train_epochs: 3.0

lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
EOF

# 3. é–‹å§‹è¨“ç·´
llamafactory-cli train minecraft_lora_config.yaml

# 4. åˆä½µ LoRA æ¬Šé‡
llamafactory-cli export \
  --model_name_or_path meta-llama/Llama-3.1-8B \
  --adapter_name_or_path minecraft_agent_lora \
  --export_dir minecraft_agent_merged

# 5. è½‰æ›ç‚º Ollama æ ¼å¼
ollama create minecraft-agent-trained \
  -f <(echo "FROM ./minecraft_agent_merged")
```

### æ–¹æ³•ä¸‰ï¼šä½¿ç”¨å¼·åŒ–å­¸ç¿’ï¼ˆé«˜éšï¼‰

```python
# rl_trainer.py - å¼·åŒ–å­¸ç¿’è¨“ç·´è…³æœ¬
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback

class MinecraftAgentEnv(gym.Env):
    """è‡ªå®šç¾© Minecraft ç’°å¢ƒ"""
    def __init__(self):
        super().__init__()
        # å®šç¾©å‹•ä½œç©ºé–“ï¼ˆæŒ–æ˜ã€ç§»å‹•ã€è£½ä½œç­‰ï¼‰
        self.action_space = gym.spaces.Discrete(10)
        # å®šç¾©è§€å¯Ÿç©ºé–“ï¼ˆå‘¨åœæ–¹å¡Šã€ç”Ÿå‘½å€¼ç­‰ï¼‰
        self.observation_space = gym.spaces.Box(...)
    
    def step(self, action):
        # åŸ·è¡Œå‹•ä½œï¼Œè¿”å›çå‹µ
        reward = self.calculate_reward(action)
        return obs, reward, done, info
    
    def calculate_reward(self, action):
        # çå‹µå‡½æ•¸è¨­è¨ˆ
        reward = 0
        if action == "collect_wood":
            reward += 10
        if agent.health > 15:
            reward += 5
        if agent.has_shelter:
            reward += 20
        return reward

# å‰µå»ºç’°å¢ƒä¸¦è¨“ç·´
env = MinecraftAgentEnv()
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=100000)
model.save("minecraft_rl_agent")
```

---

## æ€§èƒ½å„ªåŒ–

### 1. ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆæ›´å¿«æ¨ç†ï¼‰

```bash
# ä¸‹è¼‰ 4-bit é‡åŒ–ç‰ˆæœ¬ï¼ˆæ›´å¿«ï¼Œè¼ƒå°ï¼‰
ollama pull llama3.1:8b-q4_0

# æˆ– 8-bit é‡åŒ–ï¼ˆè³ªé‡æ›´å¥½ï¼‰
ollama pull llama3.1:8b-q8_0

# æ›´æ–° .env
LLM_MODEL=llama3.1:8b-q4_0
```

### 2. GPU åŠ é€Ÿ

```bash
# æª¢æŸ¥ GPU æ”¯æŒ
nvidia-smi

# Ollama æœƒè‡ªå‹•ä½¿ç”¨ GPU
# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…æ³
watch -n 1 nvidia-smi
```

### 3. èª¿æ•´ä¸¦è¡Œåƒæ•¸

ç·¨è¼¯ `.env`ï¼š
```bash
# æ¸›å°‘ä¸¦è¡Œè«‹æ±‚ï¼ˆé¿å… OOMï¼‰
LLM_NUM_PARALLEL=1

# èª¿æ•´ä¸Šä¸‹æ–‡çª—å£
LLM_NUM_CTX=4096

# èª¿æ•´æ‰¹æ¬¡å¤§å°
LLM_BATCH_SIZE=512
```

### 4. ä½¿ç”¨æ›´å¿«çš„æ¡æ¨£ç­–ç•¥

```bash
# å‰µå»ºå¿«é€Ÿæ¨ç†é…ç½®
cat > FastAgent.modelfile << 'EOF'
FROM llama3.1:8b-q4_0

PARAMETER temperature 0.5
PARAMETER top_p 0.8
PARAMETER num_predict 200
PARAMETER stop "<|eot_id|>"
EOF

ollama create fast-agent -f FastAgent.modelfile
```

---

## æ•…éšœæ’é™¤

### å•é¡Œ 1: Ollama ç„¡æ³•å•Ÿå‹•

```bash
# æª¢æŸ¥ç«¯å£ä½”ç”¨
lsof -i :11434

# æŸ¥çœ‹è©³ç´°éŒ¯èª¤
ollama serve --debug

# é‡ç½® Ollama
rm -rf ~/.ollama
ollama pull llama3.1:8b
```

### å•é¡Œ 2: Docker ç„¡æ³•é€£æ¥ Ollama

```bash
# ç¢ºèª host.docker.internal å¯ç”¨
docker run --rm alpine ping -c 3 host.docker.internal

# æˆ–ä½¿ç”¨å®¿ä¸»æ©Ÿ IP
ip addr show docker0 | grep inet
# ä¿®æ”¹ .env
OPENAI_API_BASE=http://172.17.0.1:11434/v1
```

### å•é¡Œ 3: æ¨¡å‹æ¨ç†å¤ªæ…¢

```bash
# 1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull llama3.2:3b

# 2. ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬
ollama pull llama3.1:8b-q4_0

# 3. å¢åŠ  GPU è¨˜æ†¶é«”
export OLLAMA_GPU_MEMORY=8GB

# 4. æ¸›å°‘ä¸Šä¸‹æ–‡é•·åº¦
# åœ¨ .env ä¸­è¨­ç½®
LLM_NUM_CTX=2048
```

### å•é¡Œ 4: è¨˜æ†¶é«”ä¸è¶³

```bash
# ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨
watch -n 1 free -h

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull phi3:mini

# æˆ–é™åˆ¶ Ollama è¨˜æ†¶é«”
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NUM_PARALLEL=1
```

---

## æ¨¡å‹æ¯”è¼ƒæ¸¬è©¦

å‰µå»ºæ¸¬è©¦è…³æœ¬ä¾†æ¯”è¼ƒä¸åŒæ¨¡å‹ï¼š

```bash
# test_models.sh
#!/bin/bash

MODELS=("llama3.1:8b" "mistral:7b" "qwen2.5:7b" "phi3:mini")

for model in "${MODELS[@]}"; do
    echo "æ¸¬è©¦æ¨¡å‹: $model"
    time ollama run $model "æˆ‘åœ¨ Minecraft ä¸­ï¼Œçœ‹åˆ°é è™•æœ‰æ‘èŠã€‚æˆ‘æ‡‰è©²åšä»€éº¼ï¼Ÿ" \
        | head -n 20
    echo "---"
done
```

---

## ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²

### ä½¿ç”¨ Ollama ä¼ºæœå™¨æ¨¡å¼

```yaml
# docker-compose.override.yml
version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - ai-net
  
  ai-bot:
    depends_on:
      - ollama
    environment:
      OPENAI_API_BASE: http://ollama:11434/v1
      LLM_MODEL: llama3.1:8b

volumes:
  ollama-data:
```

å•Ÿå‹•ï¼š
```bash
# ä¸‹è¼‰æ¨¡å‹åˆ° Ollama å®¹å™¨
docker-compose exec ollama ollama pull llama3.1:8b

# é‡å•Ÿ AI Agent
docker-compose restart ai-bot
```

---

## æ¨è–¦é…ç½®æ–¹æ¡ˆ

### æ–¹æ¡ˆ Aï¼šå…¥é–€ç´šï¼ˆ8GB RAMï¼Œç„¡ GPUï¼‰
```bash
# .env
OPENAI_API_BASE=http://host.docker.internal:11434/v1
LLM_MODEL=phi3:mini
LLM_NUM_CTX=2048
LLM_TEMPERATURE=0.7
```

### æ–¹æ¡ˆ Bï¼šæ¨™æº–ç´šï¼ˆ16GB RAM + RTX 3060ï¼‰
```bash
# .env
OPENAI_API_BASE=http://host.docker.internal:11434/v1
LLM_MODEL=llama3.1:8b-q4_0
LLM_NUM_CTX=4096
LLM_TEMPERATURE=0.8
```

### æ–¹æ¡ˆ Cï¼šå°ˆæ¥­ç´šï¼ˆ32GB+ RAM + RTX 4090ï¼‰
```bash
# .env
OPENAI_API_BASE=http://host.docker.internal:11434/v1
LLM_MODEL=llama3.1:70b
LLM_NUM_CTX=8192
LLM_TEMPERATURE=0.9
```

---

## ä¸‹ä¸€æ­¥

1. âœ… å®‰è£ Ollama
2. âœ… ä¸‹è¼‰é©åˆçš„æ¨¡å‹
3. âœ… é…ç½® Project Observer
4. âœ… æ¸¬è©¦æœ¬åœ° AI
5. ğŸ“ˆ æ”¶é›†æ•¸æ“šæº–å‚™å¾®èª¿
6. ğŸ¯ è¨“ç·´å°ˆå±¬æ¨¡å‹

å®Œå…¨æ“ºè„« OpenAIï¼Œæ“æœ‰è‡ªå·±çš„ Minecraft AIï¼ğŸš€

---

## åƒè€ƒè³‡æº

- [Ollama å®˜æ–¹æ–‡æª”](https://ollama.com/docs)
- [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)
- [Hugging Face æ¨¡å‹åº«](https://huggingface.co/models)
- [Stable Baselines3](https://stable-baselines3.readthedocs.io/)
