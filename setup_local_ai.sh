#!/bin/bash

# Project Observer - æœ¬åœ° AI ä¸€éµè¨­ç½®è…³æœ¬
# å®Œå…¨ä¸éœ€è¦ OpenAI API

echo "ðŸ§  Project Observer - æœ¬åœ° AI è¨­ç½®"
echo "===================================="
echo ""

# æª¢æŸ¥ Ollama æ˜¯å¦å·²å®‰è£
if command -v ollama &> /dev/null; then
    echo "âœ… Ollama å·²å®‰è£"
    ollama --version
else
    echo "ðŸ“¦ å®‰è£ Ollama..."
    
    if [[ "$OSTYPE" == "linux-gnu"* ]]; then
        curl -fsSL https://ollama.com/install.sh | sh
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        echo "è«‹å¾žä»¥ä¸‹ç¶²å€ä¸‹è¼‰ Ollama for macOS:"
        echo "https://ollama.com/download/mac"
        exit 1
    else
        echo "âŒ ä¸æ”¯æŒçš„æ“ä½œç³»çµ±: $OSTYPE"
        echo "è«‹æ‰‹å‹•å®‰è£ Ollama: https://ollama.com"
        exit 1
    fi
fi

echo ""
echo "ðŸ” é¸æ“‡è¦ä½¿ç”¨çš„æ¨¡åž‹:"
echo "  1) Llama 3.1 8B    - æŽ¨è–¦ï¼Œå¹³è¡¡æ€§èƒ½ (4.7GB)"
echo "  2) Llama 3.2 3B    - æ›´å¿«ï¼Œé©åˆä½Žé…ç½® (2GB)"
echo "  3) Mistral 7B      - é«˜è³ªé‡è¼¸å‡º (4.1GB)"
echo "  4) Qwen 2.5 7B     - ä¸­æ–‡å„ªåŒ– (4.4GB)"
echo "  5) Phi-3 Mini      - è¶…è¼•é‡ç´š (2.3GB)"
echo ""

read -p "è«‹é¸æ“‡ (1-5ï¼Œé»˜èª 1): " model_choice
model_choice=${model_choice:-1}

case $model_choice in
    1)
        MODEL="llama3.1:8b"
        MODEL_SIZE="4.7GB"
        ;;
    2)
        MODEL="llama3.2:3b"
        MODEL_SIZE="2GB"
        ;;
    3)
        MODEL="mistral:7b"
        MODEL_SIZE="4.1GB"
        ;;
    4)
        MODEL="qwen2.5:7b"
        MODEL_SIZE="4.4GB"
        ;;
    5)
        MODEL="phi3:mini"
        MODEL_SIZE="2.3GB"
        ;;
    *)
        echo "âŒ ç„¡æ•ˆé¸é …ï¼Œä½¿ç”¨é»˜èªæ¨¡åž‹"
        MODEL="llama3.1:8b"
        MODEL_SIZE="4.7GB"
        ;;
esac

echo ""
echo "ðŸ“¥ ä¸‹è¼‰æ¨¡åž‹: $MODEL ($MODEL_SIZE)"
echo "   é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜æ™‚é–“..."
echo ""

ollama pull $MODEL

if [ $? -eq 0 ]; then
    echo ""
    echo "âœ… æ¨¡åž‹ä¸‹è¼‰å®Œæˆï¼"
else
    echo "âŒ æ¨¡åž‹ä¸‹è¼‰å¤±æ•—"
    exit 1
fi

# å•Ÿå‹• Ollama æœå‹™
echo ""
echo "ðŸš€ å•Ÿå‹• Ollama æœå‹™..."

# æª¢æŸ¥æ˜¯å¦å·²ç¶“åœ¨é‹è¡Œ
if pgrep -x "ollama" > /dev/null; then
    echo "âœ… Ollama æœå‹™å·²åœ¨é‹è¡Œ"
else
    nohup ollama serve > ollama.log 2>&1 &
    sleep 3
    echo "âœ… Ollama æœå‹™å·²å•Ÿå‹•"
fi

# é…ç½® .env æ–‡ä»¶
echo ""
echo "âš™ï¸  é…ç½® Project Observer..."

if [ ! -f .env ]; then
    cp .env.example .env
fi

# æ›´æ–° .env æ–‡ä»¶
cat > .env << EOF
# ========================================
# æœ¬åœ° AI é…ç½® (ä½¿ç”¨ Ollama)
# ========================================

OPENAI_API_BASE=http://host.docker.internal:11434/v1
LLM_MODEL=$MODEL
OPENAI_API_KEY=ollama

# LLM åƒæ•¸
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1000
LLM_NUM_CTX=4096

# Agent é…ç½®
BOT_USERNAME=Agent_001
LOG_LEVEL=INFO

# Minecraft é…ç½®
MC_VERSION=1.20.1
MC_DIFFICULTY=normal
MC_MAX_MEMORY=2G
EOF

echo "âœ… é…ç½®æ–‡ä»¶å·²æ›´æ–°"

# æ¸¬è©¦æ¨¡åž‹
echo ""
echo "ðŸ§ª æ¸¬è©¦æ¨¡åž‹..."
echo ""

TEST_RESPONSE=$(ollama run $MODEL "ä½ æ˜¯ä¸€å€‹ Minecraft AIã€‚çœ‹åˆ°æ¨¹æž—ï¼Œä½ æœƒåšä»€éº¼ï¼Ÿ" | head -n 3)

if [ ! -z "$TEST_RESPONSE" ]; then
    echo "âœ… æ¨¡åž‹æ¸¬è©¦æˆåŠŸï¼"
    echo ""
    echo "æ¨¡åž‹å›žæ‡‰:"
    echo "$TEST_RESPONSE"
else
    echo "âš ï¸  æ¨¡åž‹æ¸¬è©¦æœªè¿”å›žçµæžœï¼Œä½†å¯èƒ½ä»ç„¶å¯ç”¨"
fi

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… æœ¬åœ° AI è¨­ç½®å®Œæˆï¼"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ðŸ“Š é…ç½®æ‘˜è¦:"
echo "  â€¢ æ¨¡åž‹: $MODEL"
echo "  â€¢ API åœ°å€: http://host.docker.internal:11434"
echo "  â€¢ ç„¡éœ€ OpenAI API Key"
echo ""
echo "ðŸš€ ä¸‹ä¸€æ­¥:"
echo "  1. å•Ÿå‹•ç³»çµ±: ./start.sh æˆ– make start"
echo "  2. è¨ªå• Dashboard: http://localhost:8501"
echo "  3. è§€å¯Ÿ AI ä½¿ç”¨æœ¬åœ°æ¨¡åž‹é€²è¡Œæ€è€ƒ"
echo ""
echo "ðŸ“– æ›´å¤šä¿¡æ¯: cat LOCAL_AI_GUIDE.md"
echo ""
echo "ðŸ’¡ æç¤º:"
echo "  â€¢ æŸ¥çœ‹ Ollama æ—¥èªŒ: tail -f ollama.log"
echo "  â€¢ æ¸¬è©¦æ¨¡åž‹: ollama run $MODEL"
echo "  â€¢ ä¸‹è¼‰æ›´å¤šæ¨¡åž‹: ollama pull <model>"
echo ""
